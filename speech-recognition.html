<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Speech Recognition</title>
    <style>
      body {
        font-family: Arial, sans-serif;
        max-width: 800px;
        margin: 0 auto;
        padding: 20px;
        background-color: #f5f5f5;
      }
      .container {
        background-color: white;
        border-radius: 10px;
        padding: 20px;
        box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
      }
      h1 {
        color: #333;
        text-align: center;
      }
      .controls {
        display: flex;
        justify-content: center;
        margin: 20px 0;
      }
      button {
        background-color: #4a90e2;
        color: white;
        border: none;
        padding: 10px 20px;
        border-radius: 5px;
        cursor: pointer;
        font-size: 16px;
        margin: 0 10px;
      }
      button:hover {
        background-color: #357ab8;
      }
      button.stop {
        background-color: #e25c4a;
      }
      button.stop:hover {
        background-color: #b84735;
      }
      .status {
        text-align: center;
        margin: 10px 0;
        font-weight: bold;
        color: #666;
      }
      .transcript {
        margin-top: 20px;
        padding: 15px;
        background-color: #f9f9f9;
        border-radius: 5px;
        min-height: 100px;
        border: 1px solid #ddd;
      }
      .recording-indicator {
        display: inline-block;
        width: 12px;
        height: 12px;
        background-color: #e25c4a;
        border-radius: 50%;
        margin-right: 10px;
        animation: pulse 1.5s infinite;
      }
      @keyframes pulse {
        0% {
          opacity: 1;
        }
        50% {
          opacity: 0.3;
        }
        100% {
          opacity: 1;
        }
      }
    </style>
  </head>
  <body>
    <div class="container">
      <h1>Speech Recognition</h1>

      <div class="controls">
        <button id="startBtn">Start Recording</button>
        <button id="stopBtn" class="stop" disabled>Stop Recording</button>
      </div>

      <div class="status" id="status">Ready</div>

      <div class="transcript" id="transcript">
        <p>Transcript will appear here...</p>
      </div>
    </div>

    <script>
      // Speech recognition variables
      let recognition = null;
      let isRecording = false;
      let transcript = "";

      // DOM elements
      const startBtn = document.getElementById("startBtn");
      const stopBtn = document.getElementById("stopBtn");
      const statusEl = document.getElementById("status");
      const transcriptEl = document.getElementById("transcript");

      // Initialize speech recognition
      function initSpeechRecognition() {
        const SpeechRecognition =
          window.SpeechRecognition || window.webkitSpeechRecognition;

        if (!SpeechRecognition) {
          updateStatus("Error: Web Speech API not supported in this browser");
          return false;
        }

        recognition = new SpeechRecognition();
        recognition.continuous = true;
        recognition.interimResults = true;
        recognition.lang = "en-US";

        // Set up event handlers
        recognition.onstart = () => {
          console.log("Speech recognition started");
          isRecording = true;
          updateStatus("Listening...", true);
          startBtn.disabled = true;
          stopBtn.disabled = false;
        };

        recognition.onresult = (event) => {
          let finalTranscript = "";
          let interimTranscript = "";

          for (let i = event.resultIndex; i < event.results.length; i++) {
            const result = event.results[i];
            const text = result[0].transcript;

            if (result.isFinal) {
              finalTranscript += text;
              console.log("Final transcript:", text);
            } else {
              interimTranscript += text;
              console.log("Interim transcript:", text);
            }
          }

          if (finalTranscript) {
            transcript += finalTranscript + " ";
          }

          updateTranscript(transcript, interimTranscript);
        };

        recognition.onerror = (event) => {
          console.error("Speech recognition error:", event.error);
          updateStatus(`Error: ${event.error}`);

          if (event.error === "not-allowed") {
            updateStatus(
              "Error: Microphone access denied. Please allow microphone access in your browser settings."
            );
            stopRecording();
          }
        };

        recognition.onend = () => {
          console.log("Speech recognition ended");

          if (isRecording) {
            // Try to restart if we're still supposed to be recording
            try {
              recognition.start();
              console.log("Recognition restarted");
            } catch (err) {
              console.error("Failed to restart recognition:", err);
              stopRecording();
            }
          }
        };

        return true;
      }

      // Start recording
      async function startRecording() {
        if (isRecording) return;

        try {
          // Request microphone permission
          console.log("Requesting microphone permission...");
          await navigator.mediaDevices.getUserMedia({ audio: true });
          console.log("Microphone permission granted");

          // Initialize speech recognition if not already done
          if (!recognition && !initSpeechRecognition()) {
            return;
          }

          // Start recognition
          recognition.start();
        } catch (err) {
          console.error("Error starting recording:", err);
          updateStatus(
            "Error: Microphone access denied. Please allow microphone access in your browser settings."
          );
        }
      }

      // Stop recording
      function stopRecording() {
        if (!isRecording) return;

        if (recognition) {
          try {
            recognition.stop();
            console.log("Recognition stopped");
          } catch (err) {
            console.error("Error stopping recognition:", err);
          }
        }

        isRecording = false;
        updateStatus("Stopped");
        startBtn.disabled = false;
        stopBtn.disabled = true;
      }

      // Update status display
      function updateStatus(message, isRecording = false) {
        if (isRecording) {
          statusEl.innerHTML = `<span class="recording-indicator"></span>${message}`;
        } else {
          statusEl.textContent = message;
        }
      }

      // Update transcript display
      function updateTranscript(final, interim = "") {
        if (final || interim) {
          transcriptEl.innerHTML = `<p>${final}<span style="color: #999;">${interim}</span></p>`;

          // Send transcript to extension
          if (final) {
            sendTranscriptToExtension(final);
          }
        } else {
          transcriptEl.innerHTML = "<p>Transcript will appear here...</p>";
        }
      }

      // Send transcript to extension
      function sendTranscriptToExtension(text) {
        // Check if we're in a Chrome extension context
        if (chrome && chrome.runtime) {
          try {
            chrome.runtime.sendMessage(
              {
                type: "speech_transcript",
                text: text,
                timestamp: Date.now(),
              },
              (response) => {
                console.log("Sent transcript to extension:", response);
              }
            );
          } catch (err) {
            console.error("Error sending transcript to extension:", err);
          }
        }
      }

      // Event listeners
      startBtn.addEventListener("click", startRecording);
      stopBtn.addEventListener("click", stopRecording);

      // Initialize
      updateStatus("Ready");
    </script>
  </body>
</html>
