import React, { useState, useEffect, useRef } from "react";
import {
  Mic,
  ChevronDown,
  ChevronUp,
  Waves,
  Maximize2,
  Minimize2,
} from "lucide-react";
import {
  diarizationService,
  DiarizationResult,
  Segment,
} from "../../services/diarization";

// --- Web Speech API Setup ---
const SpeechRecognition =
  (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
const recognition = SpeechRecognition ? new SpeechRecognition() : null;

if (recognition) {
  recognition.continuous = true;
  recognition.interimResults = true;
  recognition.lang = "en-US";
}
// --- End Web Speech API Setup ---

interface RecordingMicProps {
  accentColor: string;
  textColor: string;
  onStop: (finalTranscript: string) => void;
  tasks?: string[];
}

export const RecordingMic: React.FC<RecordingMicProps> = ({
  accentColor,
  textColor,
  onStop,
  tasks = [],
}) => {
  const [showTranscript, setShowTranscript] = useState(true);
  const [showTasks, setShowTasks] = useState(true);
  const [interimTranscript, setInterimTranscript] = useState("");
  const [lines, setLines] = useState<string[]>([]); // State for transcript lines
  const [audioLevel, setAudioLevel] = useState(0); // State for audio level (0-1)
  const [diarizationConnected, setDiarizationConnected] = useState(false);
  const [diarizationResults, setDiarizationResults] = useState<Segment[]>([]);
  const [showDiarization, setShowDiarization] = useState(true);
  const [isTabAudio, setIsTabAudio] = useState(true); // Always use tab audio for diarization

  // Store custom speaker names
  const [speakerNames, setSpeakerNames] = useState<Record<string, string>>({});

  // Full screen state
  const [transcriptFullScreen, setTranscriptFullScreen] = useState(false);
  const [diarizationFullScreen, setDiarizationFullScreen] = useState(false);

  // State for editing speaker name
  const [editingSpeaker, setEditingSpeaker] = useState<string | null>(null);
  const [editingName, setEditingName] = useState("");

  const MAX_TRANSCRIPT_LINES = 10; // Keep more lines in state for smoother scroll history
  const CHUNK_DURATION_MS = 1000; // 1 second per chunk - much more frequent updates
  const transcriptContainerRef = useRef<HTMLDivElement>(null); // Ref for scrolling
  const diarizationContainerRef = useRef<HTMLDivElement>(null); // Ref for diarization results
  const recognitionRef = useRef(recognition);

  // Refs for Web Audio API objects
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const sourceRef = useRef<MediaStreamAudioSourceNode | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const animationFrameRef = useRef<number | null>(null);

  // Refs for MediaRecorder and diarization
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const chunkIntervalRef = useRef<NodeJS.Timeout | null>(null);

  // Function to clean up audio resources
  const cleanupAudioResources = () => {
    console.log("Cleaning up audio resources...");

    // Stop MediaRecorder and clear interval
    if (mediaRecorderRef.current) {
      try {
        if (mediaRecorderRef.current.state !== "inactive") {
          mediaRecorderRef.current.stop();
        }
        mediaRecorderRef.current = null;
        console.log("MediaRecorder stopped.");
      } catch (e) {
        console.error("Error stopping MediaRecorder:", e);
      }
    }

    if (chunkIntervalRef.current) {
      clearInterval(chunkIntervalRef.current);
      chunkIntervalRef.current = null;
      console.log("Chunk interval cleared.");
    }

    // Disconnect Web Audio Nodes
    if (sourceRef.current) {
      sourceRef.current.disconnect();
      sourceRef.current = null;
      console.log("Audio source disconnected.");
    }
    analyserRef.current = null; // No need to disconnect analyser explicitly

    // Close Audio Context
    if (audioContextRef.current && audioContextRef.current.state !== "closed") {
      audioContextRef.current
        .close()
        .then(() => {
          console.log("Audio context closed.");
          audioContextRef.current = null;
        })
        .catch((e) => console.error("Error closing audio context:", e));
    } else {
      audioContextRef.current = null;
    }

    // Stop Media Stream Tracks
    if (streamRef.current) {
      streamRef.current.getTracks().forEach((track) => {
        track.stop();
        console.log(`Media stream track stopped: ${track.kind}`);
      });
      streamRef.current = null;
    }
  };

  // Effect for speech recognition setup (runs once on mount)
  useEffect(() => {
    const currentRecognition = recognitionRef.current;

    // --- Web Speech API Logic ---
    const setupSpeechRecognition = () => {
      if (!currentRecognition) {
        console.error("Speech Recognition API not supported.");
        return;
      }

      // Always start Web Speech API for microphone transcription

      // Standard Web Speech API setup for microphone audio
      currentRecognition.onresult = (event: any) => {
        let interim = "";
        let final = "";
        for (let i = event.resultIndex; i < event.results.length; ++i) {
          if (event.results[i].isFinal) {
            final += event.results[i][0].transcript;
          } else {
            interim += event.results[i][0].transcript;
          }
        }
        setInterimTranscript(interim); // Update interim transcript
        if (final && final.trim()) {
          // Add the new final segment as a new line
          setLines((prevLines) => {
            const newLines = [...prevLines, final.trim()];
            // Keep the last MAX_TRANSCRIPT_LINES
            return newLines.slice(
              Math.max(newLines.length - MAX_TRANSCRIPT_LINES, 0)
            );
          });
          // We clear interim here because a final result just arrived
          setInterimTranscript("");
        }
      };
      currentRecognition.onerror = (event: any) => {
        console.error("Speech recognition error:", event.error);
      };
      currentRecognition.onend = () => {
        console.log("Speech recognition ended.");
        // Don't automatically stop audio analysis here, wait for explicit stop button
      };
      try {
        // Only start Web Speech API for microphone audio
        currentRecognition.start();
        console.log("Speech recognition started.");
      } catch (e) {
        console.error("Error starting speech recognition:", e);
      }
    };

    // Start speech recognition
    setupSpeechRecognition();

    // Cleanup function for speech recognition
    return () => {
      // Stop Speech Recognition
      if (currentRecognition) {
        try {
          currentRecognition.stop();
          console.log("Speech recognition stopped on cleanup.");
          currentRecognition.onresult = null;
          currentRecognition.onerror = null;
          currentRecognition.onend = null;
        } catch (e) {
          console.error("Error stopping speech recognition on cleanup:", e);
        }
      }
    };
  }, []);

  // Effect for audio capture and diarization (runs on mount and when isTabAudio changes)
  useEffect(() => {
    let localStream: MediaStream | null = null; // Keep track of the stream locally for cleanup
    const currentRecognition = recognitionRef.current; // Get the current recognition instance

    // Clean up any existing audio resources before setting up new ones
    cleanupAudioResources();

    // --- Web Speech API Logic ---
    const setupSpeechRecognition = () => {
      if (!currentRecognition) {
        console.error("Speech Recognition API not supported.");
        return;
      }
      currentRecognition.onresult = (event: any) => {
        let interim = "";
        let final = "";
        for (let i = event.resultIndex; i < event.results.length; ++i) {
          if (event.results[i].isFinal) {
            final += event.results[i][0].transcript;
          } else {
            interim += event.results[i][0].transcript;
          }
        }
        setInterimTranscript(interim); // Update interim transcript
        if (final && final.trim()) {
          // Add the new final segment as a new line
          setLines((prevLines) => {
            const newLines = [...prevLines, final.trim()];
            // Keep the last MAX_TRANSCRIPT_LINES
            return newLines.slice(
              Math.max(newLines.length - MAX_TRANSCRIPT_LINES, 0)
            );
          });
          // We clear interim here because a final result just arrived
          setInterimTranscript("");
        }
      };
      currentRecognition.onerror = (event: any) => {
        console.error("Speech recognition error:", event.error);
      };
      currentRecognition.onend = () => {
        console.log("Speech recognition ended.");
        // Don't automatically stop audio analysis here, wait for explicit stop button
      };
      try {
        currentRecognition.start();
        console.log("Speech recognition started.");
      } catch (e) {
        console.error("Error starting speech recognition:", e);
      }
    };
    // --- End Web Speech API Logic ---

    // --- Audio Capture Logic ---
    const setupAudioAnalysis = async () => {
      try {
        // Always capture microphone audio for Web Speech API
        const micStream = await navigator.mediaDevices.getUserMedia({
          audio: true,
        });
        console.log("Microphone audio capture successful");

        // Store the microphone stream for Web Speech API
        localStream = micStream;

        // If tab audio is selected, capture tab audio for diarization
        let diarizationStream: MediaStream;

        if (isTabAudio) {
          try {
            // Check if chrome.tabCapture is available
            if (!chrome.tabCapture) {
              console.error("Tab capture API not available");
              throw new Error("Tab capture API not available");
            }

            // Get the current tab ID
            const [tab] = await chrome.tabs.query({
              active: true,
              currentWindow: true,
            });
            if (!tab || !tab.id) {
              console.error("No active tab found");
              throw new Error("No active tab found");
            }

            // Capture tab audio with audio output enabled
            diarizationStream = await new Promise<MediaStream>(
              (resolve, reject) => {
                chrome.tabCapture.capture(
                  {
                    audio: true,
                    video: false,
                    // This is important - it allows the audio to continue playing through the speakers
                    audioConstraints: {
                      mandatory: {
                        chromeMediaSource: "tab",
                      },
                    },
                  },
                  (capturedStream) => {
                    if (chrome.runtime.lastError) {
                      console.error(
                        "Tab capture error:",
                        chrome.runtime.lastError
                      );
                      reject(new Error(chrome.runtime.lastError.message));
                      return;
                    }
                    if (!capturedStream) {
                      console.error("Tab capture returned null stream");
                      reject(new Error("Tab capture returned null stream"));
                      return;
                    }

                    try {
                      // Create an audio context to process the stream
                      const audioContext = new (window.AudioContext ||
                        (window as any).webkitAudioContext)();

                      // Create a media stream source from the captured stream
                      const source =
                        audioContext.createMediaStreamSource(capturedStream);

                      // Create a destination that outputs to the default audio output
                      const destination =
                        audioContext.createMediaStreamDestination();

                      // Connect the source to the destination
                      source.connect(destination);

                      // Also connect the source to the audio context destination to allow audio to play through speakers
                      source.connect(audioContext.destination);

                      // Store the audio context for cleanup
                      audioContextRef.current = audioContext;

                      // Use the destination stream for processing
                      resolve(capturedStream);
                    } catch (error) {
                      console.error(
                        "Error setting up audio processing:",
                        error
                      );
                      // If there's an error, just use the captured stream directly
                      resolve(capturedStream);
                    }
                  }
                );
              }
            );

            console.log("Tab audio capture successful");
          } catch (error) {
            console.error("Error capturing tab audio:", error);
            // Fall back to microphone audio if tab audio capture fails
            console.log(
              "Falling back to microphone audio for diarization as well"
            );
            setIsTabAudio(false);
            diarizationStream = micStream;
          }
        } else {
          // Use microphone audio for diarization as well
          diarizationStream = micStream;
        }

        // Store the stream for visualization and cleanup
        streamRef.current = micStream;

        // Set up audio analysis
        const audioContext = new (window.AudioContext ||
          (window as any).webkitAudioContext)();
        audioContextRef.current = audioContext;

        const analyser = audioContext.createAnalyser();
        analyser.fftSize = 256; // Smaller FFT size for faster response
        analyserRef.current = analyser;

        // Create a source node from the diarization stream for visualization
        const source = audioContext.createMediaStreamSource(diarizationStream);
        sourceRef.current = source;
        source.connect(analyser);

        const bufferLength = analyser.frequencyBinCount;
        const dataArray = new Uint8Array(bufferLength);

        const updateLevel = () => {
          if (!analyserRef.current) return;
          analyserRef.current.getByteFrequencyData(dataArray);

          // Simple average calculation for volume level
          let sum = 0;
          for (let i = 0; i < bufferLength; i++) {
            sum += dataArray[i];
          }
          const average = sum / bufferLength;
          // Normalize the level (0-1), you might need to adjust the divisor (255)
          const normalizedLevel = Math.min(average / 128, 1); // Adjust divisor for sensitivity
          setAudioLevel(normalizedLevel);

          animationFrameRef.current = requestAnimationFrame(updateLevel);
        };

        updateLevel(); // Start the animation loop
        console.log("Audio analysis started.");

        // Setup MediaRecorder for diarization
        setupMediaRecorder(diarizationStream);
      } catch (err) {
        console.error("Error setting up audio analysis:", err);
        // Handle audio capture errors
      }
    };
    // --- End Audio Capture Logic ---

    // --- MediaRecorder Setup for Diarization ---
    const setupMediaRecorder = (stream: MediaStream) => {
      try {
        // Connect to diarization server
        diarizationService
          .connect()
          .then(() => {
            setDiarizationConnected(true);
            console.log("Connected to diarization server");

            // Set up callbacks
            diarizationService.onResult((result: DiarizationResult) => {
              if (result.segments) {
                // Sort segments by start time to ensure chronological order
                const sortedSegments = [...result.segments].sort(
                  (a, b) => a.start - b.start
                );

                // Process segments to merge consecutive segments from the same speaker
                const mergedSegments: Segment[] = [];

                sortedSegments.forEach((segment) => {
                  const lastSegment =
                    mergedSegments.length > 0
                      ? mergedSegments[mergedSegments.length - 1]
                      : null;

                  // If this segment is from the same speaker as the last one, merge them
                  if (lastSegment && lastSegment.speaker === segment.speaker) {
                    lastSegment.text = `${lastSegment.text} ${segment.text}`;
                    lastSegment.end = segment.end; // Update the end time
                  } else {
                    // Otherwise add as a new segment
                    mergedSegments.push({ ...segment });
                  }
                });

                // Append new segments to existing ones instead of replacing
                setDiarizationResults((prevResults) => {
                  // Create a new array with all previous results
                  const combinedResults = [...prevResults];

                  // Add new segments, avoiding duplicates based on time ranges
                  mergedSegments.forEach((newSegment) => {
                    // Check if this segment overlaps with any existing segment
                    const overlappingIndex = combinedResults.findIndex(
                      (existing) =>
                        (newSegment.start >= existing.start &&
                          newSegment.start <= existing.end) ||
                        (newSegment.end >= existing.start &&
                          newSegment.end <= existing.end) ||
                        (existing.start >= newSegment.start &&
                          existing.start <= newSegment.end)
                    );

                    if (overlappingIndex === -1) {
                      // No overlap, add as new segment
                      combinedResults.push(newSegment);
                    } else {
                      // If there's overlap and it's the same speaker, merge the text
                      if (
                        combinedResults[overlappingIndex].speaker ===
                        newSegment.speaker
                      ) {
                        combinedResults[overlappingIndex].text +=
                          " " + newSegment.text;
                        // Update the time range if needed
                        combinedResults[overlappingIndex].start = Math.min(
                          combinedResults[overlappingIndex].start,
                          newSegment.start
                        );
                        combinedResults[overlappingIndex].end = Math.max(
                          combinedResults[overlappingIndex].end,
                          newSegment.end
                        );
                      } else {
                        // Different speaker, add as new segment
                        combinedResults.push(newSegment);
                      }
                    }
                  });

                  // Sort by start time
                  combinedResults.sort((a, b) => a.start - b.start);

                  console.log("Updated diarization results:", combinedResults);
                  return combinedResults;
                });
              }
              if (result.error) {
                console.error("Diarization error:", result.error);
              }
            });

            diarizationService.onError((error: Error) => {
              console.error("Diarization connection error:", error);
              setDiarizationConnected(false);
            });
          })
          .catch((error) => {
            console.error("Failed to connect to diarization server:", error);
          });

        // Create MediaRecorder with appropriate settings for diarization
        // The server expects 16kHz, 16-bit, mono audio
        // We'll use audio/webm for compatibility, then convert before sending

        // Create an audio context for processing
        const audioContext = new (window.AudioContext ||
          (window as any).webkitAudioContext)({
          sampleRate: 16000, // Force 16kHz sample rate to match server expectations
        });
        audioContextRef.current = audioContext;

        // Create a script processor node to process audio data
        // Note: ScriptProcessorNode is deprecated but still widely supported
        const scriptNode = audioContext.createScriptProcessor(4096, 1, 1);

        // Create an analyser for visualization
        const analyser = audioContext.createAnalyser();
        analyser.fftSize = 256;
        analyserRef.current = analyser;

        // Create a source node from the stream for visualization
        const source = audioContext.createMediaStreamSource(stream);
        sourceRef.current = source;

        // Connect the source to the analyser and script processor
        source.connect(analyser);
        source.connect(scriptNode);
        scriptNode.connect(audioContext.destination);

        // Buffer to accumulate audio data
        const BUFFER_SIZE = 16000 * 5; // 5 seconds of audio at 16kHz
        let audioBuffer = new Float32Array(BUFFER_SIZE);
        let bufferIndex = 0;

        // Process audio data
        scriptNode.onaudioprocess = (audioProcessingEvent) => {
          // Get the input buffer
          const inputBuffer = audioProcessingEvent.inputBuffer;
          const inputData = inputBuffer.getChannelData(0);

          // Copy data to our buffer
          for (let i = 0; i < inputData.length; i++) {
            if (bufferIndex < BUFFER_SIZE) {
              audioBuffer[bufferIndex++] = inputData[i];
            }
          }

          // If buffer is full, send it to the server
          if (
            bufferIndex >= BUFFER_SIZE &&
            diarizationService.isSocketConnected()
          ) {
            // Convert to 16-bit PCM
            const pcm16 = new Int16Array(BUFFER_SIZE);
            for (let i = 0; i < BUFFER_SIZE; i++) {
              // Convert float (-1.0 to 1.0) to int16 (-32768 to 32767)
              const s = Math.max(-1, Math.min(1, audioBuffer[i]));
              pcm16[i] = s < 0 ? s * 32768 : s * 32767;
            }

            console.log(
              `Sending ${pcm16.byteLength} bytes of 16-bit PCM audio data`
            );

            // Create a blob from the PCM data
            const blob = new Blob([pcm16.buffer], { type: "audio/raw" });

            // Send to diarization server
            diarizationService
              .sendAudioChunk(blob)
              .then(() => {
                console.log(
                  "Audio data sent to diarization server successfully"
                );
              })
              .catch((error) => {
                console.error("Error sending audio data:", error);
              });

            // Reset buffer index to start filling again
            bufferIndex = 0;
          }
        };

        // Store the script processor node for cleanup
        mediaRecorderRef.current = scriptNode as any; // Store in mediaRecorderRef for cleanup
      } catch (error) {
        console.error("Error setting up MediaRecorder:", error);
      }
    };
    // --- End MediaRecorder Setup ---

    // Start both APIs
    setupSpeechRecognition();
    setupAudioAnalysis();

    // --- Cleanup Function ---
    return () => {
      console.log("Cleaning up RecordingMic...");

      // Stop Speech Recognition
      if (currentRecognition) {
        try {
          currentRecognition.stop();
          console.log("Speech recognition stopped on cleanup.");
          currentRecognition.onresult = null;
          currentRecognition.onerror = null;
          currentRecognition.onend = null;
        } catch (e) {
          console.error("Error stopping speech recognition on cleanup:", e);
        }
      }

      // Stop Audio Analysis Animation Loop
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
        animationFrameRef.current = null;
        console.log("Audio analysis animation stopped.");
      }

      // Stop MediaRecorder and clear interval
      if (mediaRecorderRef.current) {
        try {
          if (mediaRecorderRef.current.state !== "inactive") {
            mediaRecorderRef.current.stop();
          }
          mediaRecorderRef.current = null;
          console.log("MediaRecorder stopped.");
        } catch (e) {
          console.error("Error stopping MediaRecorder:", e);
        }
      }

      if (chunkIntervalRef.current) {
        clearInterval(chunkIntervalRef.current);
        chunkIntervalRef.current = null;
        console.log("Chunk interval cleared.");
      }

      // Disconnect from diarization server
      if (diarizationService.isSocketConnected()) {
        diarizationService.disconnect();
        console.log("Disconnected from diarization server.");
      }

      // Disconnect Web Audio Nodes
      if (sourceRef.current) {
        sourceRef.current.disconnect();
        sourceRef.current = null;
        console.log("Audio source disconnected.");
      }
      analyserRef.current = null; // No need to disconnect analyser explicitly

      // Close Audio Context
      if (
        audioContextRef.current &&
        audioContextRef.current.state !== "closed"
      ) {
        audioContextRef.current
          .close()
          .then(() => {
            console.log("Audio context closed.");
            audioContextRef.current = null;
          })
          .catch((e) => console.error("Error closing audio context:", e));
      } else {
        audioContextRef.current = null;
      }

      // Stop Media Stream Tracks
      if (streamRef.current) {
        streamRef.current.getTracks().forEach((track) => {
          track.stop();
          console.log(`Media stream track stopped: ${track.kind}`);
        });
        streamRef.current = null;
      } else if (localStream) {
        // Fallback if streamRef wasn't set but localStream was acquired
        localStream.getTracks().forEach((track) => {
          track.stop();
          console.log(`Media stream track stopped (fallback): ${track.kind}`);
        });
      }
    };
    // Run effect when isTabAudio changes
  }, [isTabAudio]);

  // Reintroduce smooth scrolling effect
  useEffect(() => {
    if (transcriptContainerRef.current) {
      // Scroll smoothly to the bottom
      transcriptContainerRef.current.scrollTo({
        top: transcriptContainerRef.current.scrollHeight,
        behavior: "smooth",
      });
    }
  }, [lines, interimTranscript]); // Scroll when lines or interim changes

  // Scroll diarization results when they update
  useEffect(() => {
    if (diarizationContainerRef.current) {
      diarizationContainerRef.current.scrollTo({
        top: diarizationContainerRef.current.scrollHeight,
        behavior: "smooth",
      });
    }
  }, [diarizationResults]);

  // Add keyboard event listener for Escape key to exit full screen
  useEffect(() => {
    const handleKeyDown = (e: KeyboardEvent) => {
      if (e.key === "Escape") {
        if (transcriptFullScreen) {
          setTranscriptFullScreen(false);
        }
        if (diarizationFullScreen) {
          setDiarizationFullScreen(false);
        }
      }
    };

    window.addEventListener("keydown", handleKeyDown);

    return () => {
      window.removeEventListener("keydown", handleKeyDown);
    };
  }, [transcriptFullScreen, diarizationFullScreen]);

  const handleStop = () => {
    // The cleanup function in useEffect handles stopping everything
    const fullTranscript = lines.join(" ") + " " + interimTranscript; // Combine lines for final output
    onStop(fullTranscript.trim()); // Call the parent's onStop
    // Note: The component will unmount after this, triggering the cleanup
  };

  const toggleTranscript = () => setShowTranscript(!showTranscript);
  const toggleTasks = () => setShowTasks(!showTasks);
  const toggleDiarization = () => setShowDiarization(!showDiarization);

  // Calculate dynamic styles based on audioLevel
  const baseScale = 1;
  const maxScaleAddition = 0.4; // Max scale increase (e.g., 1.0 + 0.4 = 1.4)
  const baseOpacity = 0.3;
  const maxOpacityAddition = 0.4; // Max opacity increase (e.g., 0.3 + 0.4 = 0.7)

  const rippleStyle1 = {
    transform: `scale(${baseScale + audioLevel * maxScaleAddition * 0.6})`, // Less sensitive ripple
    opacity: baseOpacity + audioLevel * maxOpacityAddition * 0.6,
    transition: "transform 0.1s ease-out, opacity 0.1s ease-out", // Smooth transitions
  };
  const rippleStyle2 = {
    transform: `scale(${baseScale + audioLevel * maxScaleAddition * 0.8})`, // Medium ripple
    opacity: baseOpacity + audioLevel * maxOpacityAddition * 0.8,
    transition: "transform 0.1s ease-out, opacity 0.1s ease-out",
  };
  const rippleStyle3 = {
    transform: `scale(${baseScale + audioLevel * maxScaleAddition})`, // Most sensitive ripple
    opacity: baseOpacity + audioLevel * maxOpacityAddition,
    transition: "transform 0.1s ease-out, opacity 0.1s ease-out",
  };

  return (
    <div className="d4m-flex d4m-flex-col d4m-items-center d4m-gap-6 d4m-w-full d4m-h-full d4m-p-4 d4m-overflow-y-auto">
      {/* Add content here */}
      {/* MIC BUTTON */}
      <div className="d4m-flex d4m-flex-col d4m-items-center d4m-gap-2">
        <div className="d4m-text-sm d4m-text-center d4m-text-gray-400 d4m-mb-2">
          Using both microphone for transcription and tab audio for diarization
        </div>
        <div className="d4m-relative d4m-flex d4m-items-center d4m-justify-center d4m-h-40">
          <button
            type="button"
            onClick={handleStop}
            className={`d4m-relative d4m-w-20 d4m-h-20 d4m-rounded-full d4m-bg-${accentColor}-500 d4m-text-white d4m-flex d4m-items-center d4m-justify-center d4m-shadow-lg d4m-transition-all d4m-duration-300 d4m-transform hover:d4m-scale-105 d4m-cursor-pointer focus:d4m-outline-none`}
          >
            {/* Ripple/Glow Animations - Removed d4m-animate-mic-ripple, added inline styles */}
            <span
              className={`d4m-absolute d4m-inset-0 d4m-rounded-full d4m-bg-${accentColor}-400`}
              style={rippleStyle1}
            ></span>
            <span
              className={`d4m-absolute d4m-inset-0 d4m-rounded-full d4m-bg-${accentColor}-300`}
              style={rippleStyle2}
            ></span>
            <span
              className={`d4m-absolute d4m-inset-0 d4m-rounded-full d4m-bg-${accentColor}-200`}
              style={rippleStyle3}
            ></span>
            {/* Keep the glow separate */}
            <div
              className={`d4m-absolute d4m-inset-0 d4m-rounded-full d4m-border-4 d4m-border-${accentColor}-300 d4m-opacity-70 d4m-animate-mic-glow`}
            ></div>
            <Mic className="d4m-w-8 d4m-h-8 d4m-relative d4m-z-10 d4m-animate-mic-bounce" />
          </button>
        </div>
      </div>

      {/* TRANSCRIPT SECTION */}
      <div className="d4m-w-full">
        <div className="d4m-flex d4m-justify-between d4m-items-center d4m-mb-2">
          <h2 className={`d4m-text-md d4m-font-bold ${textColor}`}>
            Live Transcript
          </h2>
          <div className="d4m-flex d4m-gap-2">
            <button
              type="button"
              onClick={() => setTranscriptFullScreen(!transcriptFullScreen)}
              className="d4m-p-1 d4m-rounded d4m-hover:bg-gray-700"
              title={transcriptFullScreen ? "Exit Full Screen" : "Full Screen"}
            >
              {transcriptFullScreen ? (
                <Minimize2 className="d4m-w-4 d4m-h-4" />
              ) : (
                <Maximize2 className="d4m-w-4 d4m-h-4" />
              )}
            </button>
            <button type="button" onClick={toggleTranscript}>
              {showTranscript ? (
                <ChevronUp className="d4m-w-4 d4m-h-4" />
              ) : (
                <ChevronDown className="d4m-w-4 d4m-h-4" />
              )}
            </button>
          </div>
        </div>
        {showTranscript && (
          <>
            {/* Fixed control bar for full screen mode */}
            {transcriptFullScreen && (
              <div
                className="transcript-full-screen d4m-fixed d4m-top-0 d4m-left-0 d4m-right-0 d4m-bg-gray-800 d4m-p-2 d4m-flex d4m-justify-between d4m-items-center d4m-border-b d4m-border-gray-700"
                style={{ zIndex: 1000 }}
              >
                <h2 className={`d4m-text-md d4m-font-bold d4m-text-white`}>
                  Live Transcript
                </h2>
                <button
                  type="button"
                  onClick={() => setTranscriptFullScreen(false)}
                  className="d4m-p-2 d4m-rounded d4m-bg-gray-700 d4m-hover:bg-gray-600"
                  title="Exit Full Screen"
                >
                  <Minimize2 className="d4m-w-5 d4m-h-5 d4m-text-white" />
                </button>
              </div>
            )}
            <div
              ref={transcriptContainerRef}
              className={`
  ${
    transcriptFullScreen
      ? "d4m-fixed d4m-inset-0 d4m-z-50 d4m-pt-12 d4m-px-4 d4m-pb-4 d4m-bg-gray-900"
      : "d4m-h-48 d4m-relative"
  }
  d4m-overflow-y-auto
  d4m-rounded-lg d4m-p-2
  d4m-bg-gradient-to-b d4m-from-gray-700/70 d4m-via-gray-800/80 d4m-to-gray-900/90
  d4m-ring-1 d4m-ring-inset d4m-ring-gray-500/20
  d4m-scrollbar-thin d4m-scrollbar-thumb-gray-400 d4m-scrollbar-track-transparent
`}
              style={
                {
                  "--mask-gradient":
                    "linear-gradient(to bottom, transparent 0%, black 20%, black 80%, transparent 100%)",
                  maskImage: transcriptFullScreen
                    ? "none"
                    : "var(--mask-gradient)",
                  WebkitMaskImage: transcriptFullScreen
                    ? "none"
                    : "var(--mask-gradient)",
                } as React.CSSProperties
              }
            >
              {/* Inner div for the actual content */}
              <div className="d4m-w-full d4m-space-y-1">
                {/* Placeholder */}
                {lines.length === 0 && !interimTranscript && (
                  <p className="d4m-text-sm d4m-text-gray-500 d4m-italic d4m-text-center">
                    Listening...
                  </p>
                )}

                {/* Finalized Lines */}
                {lines.map((line, index) => {
                  const isLastFinalized = index === lines.length - 1;
                  // Opacity based on position relative to the end (newest)
                  const lineDistanceFromEnd = lines.length - 1 - index;
                  // Removed opacity calculation for simplicity with scrolling
                  return (
                    <p
                      key={index}
                      className={`
                        d4m-text-sm d4m-text-center /* Consistent text-sm */
                        ${
                          isLastFinalized
                            ? "d4m-text-blue-300 d4m-font-medium" // Last final line is blue and medium weight
                            : "d4m-text-gray-300" // Older lines are gray
                        }
                      `}
                      // Removed inline style for opacity
                    >
                      {line}
                    </p>
                  );
                })}

                {/* Interim Transcript (appears below the last finalized line) */}
                {interimTranscript && (
                  <p className="d4m-text-sm d4m-text-blue-200/70 d4m-text-center">
                    {" "}
                    {/* Consistent text-sm */}
                    {interimTranscript}
                  </p>
                )}
              </div>
            </div>
          </>
        )}
      </div>

      {/* DIARIZATION SECTION */}
      <div className="d4m-w-full">
        <div className="d4m-flex d4m-justify-between d4m-items-center d4m-mb-2">
          <h2
            className={`d4m-text-md d4m-font-bold ${textColor} d4m-flex d4m-items-center d4m-gap-1`}
          >
            <Waves className="d4m-w-4 d4m-h-4" />
            Diarization
            {diarizationConnected && (
              <span className="d4m-w-2 d4m-h-2 d4m-bg-green-500 d4m-rounded-full d4m-ml-1"></span>
            )}
          </h2>
          <div className="d4m-flex d4m-gap-2">
            <button
              type="button"
              onClick={() => setDiarizationFullScreen(!diarizationFullScreen)}
              className="d4m-p-1 d4m-rounded d4m-hover:bg-gray-700"
              title={diarizationFullScreen ? "Exit Full Screen" : "Full Screen"}
            >
              {diarizationFullScreen ? (
                <Minimize2 className="d4m-w-4 d4m-h-4" />
              ) : (
                <Maximize2 className="d4m-w-4 d4m-h-4" />
              )}
            </button>
            <button type="button" onClick={toggleDiarization}>
              {showDiarization ? (
                <ChevronUp className="d4m-w-4 d4m-h-4" />
              ) : (
                <ChevronDown className="d4m-w-4 d4m-h-4" />
              )}
            </button>
          </div>
        </div>
        {showDiarization && (
          <React.Fragment>
            {diarizationFullScreen && (
              <div
                className="d4m-fixed d4m-top-0 d4m-left-0 d4m-right-0 d4m-bg-gray-800 d4m-p-2 d4m-flex d4m-justify-between d4m-items-center d4m-border-b d4m-border-gray-700"
                style={{ zIndex: 1000 }}
              >
                <h2 className={`d4m-text-md d4m-font-bold d4m-text-white`}>
                  Diarization
                </h2>
                <button
                  type="button"
                  onClick={() => setDiarizationFullScreen(false)}
                  className="d4m-p-2 d4m-rounded d4m-bg-gray-700 d4m-hover:bg-gray-600"
                  title="Exit Full Screen"
                >
                  <Minimize2 className="d4m-w-5 d4m-h-5 d4m-text-white" />
                </button>
              </div>
            )}
            <div
              ref={diarizationContainerRef}
              className={`
  ${
    diarizationFullScreen
      ? "d4m-fixed d4m-inset-0 d4m-z-50 d4m-pt-12 d4m-px-4 d4m-pb-4 d4m-bg-gray-900"
      : "d4m-h-48 d4m-relative"
  }
  d4m-overflow-y-auto 
  d4m-rounded-lg d4m-p-2 
  d4m-bg-gradient-to-b d4m-from-gray-700/70 d4m-via-gray-800/80 d4m-to-gray-900/90
  d4m-ring-1 d4m-ring-inset d4m-ring-gray-500/20
  d4m-scrollbar-thin d4m-scrollbar-thumb-gray-400 d4m-scrollbar-track-transparent
`}
              style={
                {
                  "--mask-gradient":
                    "linear-gradient(to bottom, transparent 0%, black 20%, black 80%, transparent 100%)",
                  maskImage: diarizationFullScreen
                    ? "none"
                    : "var(--mask-gradient)",
                  WebkitMaskImage: diarizationFullScreen
                    ? "none"
                    : "var(--mask-gradient)",
                } as React.CSSProperties
              }
            >
              {/* Inner div for the actual content */}
              <div className="d4m-w-full d4m-space-y-1">
                {/* Placeholder */}
                {diarizationResults.length === 0 && (
                  <p className="d4m-text-sm d4m-text-gray-500 d4m-italic d4m-text-center">
                    {diarizationConnected
                      ? "Waiting for diarization results..."
                      : "Connecting to diarization server..."}
                  </p>
                )}

                {/* Diarization Results */}
                {diarizationResults.map((segment, index) => (
                  <div
                    key={index}
                    className="d4m-flex d4m-gap-2 d4m-items-start"
                  >
                    {editingSpeaker === segment.speaker ? (
                      // Edit mode
                      <div className="d4m-flex d4m-items-center d4m-gap-1">
                        <input
                          type="text"
                          value={editingName}
                          onChange={(e) => setEditingName(e.target.value)}
                          onKeyDown={(e) => {
                            if (e.key === "Enter") {
                              // Save the custom name
                              setSpeakerNames({
                                ...speakerNames,
                                [segment.speaker]: editingName,
                              });
                              setEditingSpeaker(null);
                            } else if (e.key === "Escape") {
                              // Cancel editing
                              setEditingSpeaker(null);
                            }
                          }}
                          className="d4m-text-xs d4m-px-1 d4m-py-0.5 d4m-rounded d4m-bg-gray-700 d4m-text-white d4m-border d4m-border-gray-500 d4m-w-24"
                          autoFocus
                        />
                        <button
                          onClick={() => {
                            // Save the custom name
                            setSpeakerNames({
                              ...speakerNames,
                              [segment.speaker]: editingName,
                            });
                            setEditingSpeaker(null);
                          }}
                          className="d4m-text-xs d4m-px-1 d4m-py-0.5 d4m-rounded d4m-bg-green-600 d4m-text-white"
                        >
                          Save
                        </button>
                      </div>
                    ) : (
                      // Display mode
                      <button
                        onClick={() => {
                          setEditingSpeaker(segment.speaker);
                          setEditingName(
                            speakerNames[segment.speaker] || segment.speaker
                          );
                        }}
                        className={`d4m-text-xs d4m-font-bold d4m-px-1 d4m-py-0.5 d4m-rounded d4m-bg-${accentColor}-500 d4m-text-white d4m-cursor-pointer hover:d4m-bg-${accentColor}-600`}
                        title="Click to edit speaker name"
                      >
                        {speakerNames[segment.speaker] || segment.speaker}
                      </button>
                    )}
                    <p className="d4m-text-sm d4m-text-gray-300 d4m-flex-1">
                      {segment.text}
                    </p>
                  </div>
                ))}
              </div>
            </div>
          </React.Fragment>
        )}
      </div>
    </div>
  );
};
